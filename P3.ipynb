{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 1: \n",
    "\n",
    "An image feature is simply interesting or meaningful areas in an image such as the corners of a house or the peaks of a mountain. They are the most useful areas in an image to give enough information to describe the image. An image feature vector is just the numerical representation of these image features placed in a vector.  Typically many of these interest points would have to be found using something like sift rather than manually looking through an image since some points are not as intuitive as say the corners of a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: Dog vs Cat\n",
      "Image Paths: ['image_set/dog_0.jpg', 'image_set/dog_1.jpg', 'image_set/dog_2.jpg', 'image_set/dog_3.jpg', 'image_set/dog_4.jpg', 'image_set/dog_5.jpg', 'image_set/dog_6.jpg', 'image_set/dog_7.jpg', 'image_set/dog_8.jpg', 'image_set/dog_9.jpg', 'image_set/cat_0.jpg', 'image_set/cat_1.jpg', 'image_set/cat_2.jpg', 'image_set/cat_3.jpg', 'image_set/cat_4.jpg', 'image_set/cat_5.jpg', 'image_set/cat_6.jpg', 'image_set/cat_7.jpg', 'image_set/cat_8.jpg', 'image_set/cat_9.jpg']\n",
      "Labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Total Images: 20\n",
      "\n",
      "Task: Mango vs Banana\n",
      "Image Paths: ['image_set/mango_0.jpg', 'image_set/mango_1.jpg', 'image_set/mango_2.jpg', 'image_set/mango_3.jpg', 'image_set/mango_4.jpg', 'image_set/mango_5.jpg', 'image_set/mango_6.jpg', 'image_set/mango_7.jpg', 'image_set/mango_8.jpg', 'image_set/mango_9.jpg', 'image_set/banana_0.jpg', 'image_set/banana_1.jpg', 'image_set/banana_2.jpg', 'image_set/banana_3.jpg', 'image_set/banana_4.jpg', 'image_set/banana_5.jpg', 'image_set/banana_6.jpg', 'image_set/banana_7.jpg', 'image_set/banana_8.jpg', 'image_set/banana_9.jpg']\n",
      "Labels: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Total Images: 20\n",
      "\n",
      "Task: Goldfish vs Orca\n",
      "Image Paths: ['image_set/goldfish_0.jpg', 'image_set/goldfish_1.jpg', 'image_set/goldfish_2.jpg', 'image_set/goldfish_3.jpg', 'image_set/goldfish_4.jpg', 'image_set/goldfish_5.jpg', 'image_set/goldfish_6.jpg', 'image_set/goldfish_7.jpg', 'image_set/goldfish_8.jpg', 'image_set/goldfish_9.jpg', 'image_set/orca_0.jpg', 'image_set/orca_1.jpg', 'image_set/orca_2.jpg', 'image_set/orca_3.jpg', 'image_set/orca_4.jpg', 'image_set/orca_5.jpg', 'image_set/orca_6.jpg', 'image_set/orca_7.jpg', 'image_set/orca_8.jpg', 'image_set/orca_9.jpg']\n",
      "Labels: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Total Images: 20\n"
     ]
    }
   ],
   "source": [
    "#PROBLEM 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "\n",
    "class ImageDatasetLoader:\n",
    "    def __init__(self, img_dir):\n",
    "        # three 2-class tasks\n",
    "        self.tasks = [\n",
    "            {\n",
    "                'name': 'Dog vs Cat',\n",
    "                'classes': {\n",
    "                    'dog': 1,  # Class A\n",
    "                    'cat': 2   # Class B\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Mango vs Banana',\n",
    "                'classes': {\n",
    "                    'mango': 3,  # Class C\n",
    "                    'banana': 4  # Class D\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Goldfish vs Orca',\n",
    "                'classes': {\n",
    "                    'goldfish': 5,  # Class E\n",
    "                    'orca': 6       # Class F\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        self.img_dir = img_dir\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        Load images for each 2-class task\n",
    "        Returns a list of dictionaries, each containing:\n",
    "        - task name\n",
    "        - image paths\n",
    "        - labels\n",
    "        \"\"\"\n",
    "        all_task_datasets = []\n",
    "        \n",
    "        for task in self.tasks:\n",
    "            image_paths = []\n",
    "            labels = []\n",
    "            \n",
    "            #10 images for each class in the task\n",
    "            for category, label in task['classes'].items():\n",
    "                for i in range(10):\n",
    "                    filename = f\"{category}_{i}.jpg\"\n",
    "                    full_path = os.path.join(self.img_dir, filename)\n",
    "                    \n",
    "                    if os.path.exists(full_path):\n",
    "                        image_paths.append(full_path)\n",
    "                        labels.append(label)\n",
    "                    else:\n",
    "                        print(f\"{filename} not found\")\n",
    "            \n",
    "            task_dataset = {\n",
    "                'name': task['name'],\n",
    "                'image_paths': image_paths,\n",
    "                'labels': labels\n",
    "            }\n",
    "            all_task_datasets.append(task_dataset)\n",
    "        \n",
    "        return all_task_datasets\n",
    "\n",
    "#Load the datasets\n",
    "\n",
    "img_dir = \"image_set/\"\n",
    "dataset_loader = ImageDatasetLoader(img_dir)\n",
    "\n",
    "task_datasets = dataset_loader.load_datasets()\n",
    "\n",
    "# Making dataset copies so that they dont affect each other \n",
    "task_datasetsHAND = task_datasets\n",
    "task_datasetsCNN = task_datasets\n",
    "task_datasetsDINO = task_datasets\n",
    "\n",
    "#Print\n",
    "for task in task_datasets:\n",
    "    print(f\"\\nTask: {task['name']}\")\n",
    "    print(\"Image Paths:\", task['image_paths'])\n",
    "    print(\"Labels:\", task['labels'])\n",
    "    print(\"Total Images:\", len(task['image_paths']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printed above is the array of each image in the dataset followed by an array with the class labels of each respective image. This is done for A vs B, C vs D, and E vs F.\n",
    "\n",
    "Classes 1, 2, 3, 4, 5, and 6 correspond with A, B, C, D, E, F respectively where\n",
    "A = dog, \n",
    "B = cat, \n",
    "C = mango, \n",
    "D = banana, \n",
    "E = goldfish, \n",
    "F = orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nihal/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\nihal/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\nihal/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\nihal/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "Task: Dog vs Cat\n",
      "Feature shape: (20, 384)\n",
      "Labels shape: 20\n",
      "\n",
      "Task: Mango vs Banana\n",
      "Feature shape: (20, 384)\n",
      "Labels shape: 20\n",
      "\n",
      "Task: Goldfish vs Orca\n",
      "Feature shape: (20, 384)\n",
      "Labels shape: 20\n"
     ]
    }
   ],
   "source": [
    "#PROBLEM 3\n",
    "\n",
    "# This code finds all the feature vectors using DINOv2.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "class DinoV2FeatureExtractor:\n",
    "\n",
    "    #initializing\n",
    "    def __init__(self, model_name='dinov2_vits14'):\n",
    "\n",
    "        try:\n",
    "            self.model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "            self.model.eval()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DinoV2 model: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # this is the standard transformation for DinoV2\n",
    "        print(\"---\")\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, image_path):\n",
    "\n",
    "        # Extract features from a single image\n",
    "        # Args:\n",
    "        #     Path to the image file\n",
    "        # Returns:\n",
    "        #     Extracted feature vector\n",
    "\n",
    "        try:\n",
    "            # Open and transform the image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            input_tensor = self.transform(image).unsqueeze(0)\n",
    "            \n",
    "            #Extract features\n",
    "            with torch.no_grad():\n",
    "                features = self.model.get_intermediate_layers(input_tensor, n=1)[0]\n",
    "                features = features.mean(1).squeeze()  \n",
    "            \n",
    "            return features.numpy()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"failed for {image_path}: {e}\")\n",
    "            return np.zeros(768) \n",
    "\n",
    "\n",
    "#Extract DinoV2 features for all tasks\n",
    "\n",
    "def extract_dinov2_features(task_datasetsDINO):\n",
    "\n",
    "    #initializing feature vec\n",
    "    extractor = DinoV2FeatureExtractor()\n",
    "    \n",
    "    task_features = []\n",
    "    \n",
    "    for task in task_datasetsDINO:\n",
    "        #extracting features for tast\n",
    "        features = []\n",
    "        for image_path in task['image_paths']:\n",
    "            task_feature = extractor.extract_features(image_path)\n",
    "            features.append(task_feature)\n",
    "        \n",
    "        #creating a task feature dictionary\n",
    "        task_feature_dict = {\n",
    "            'name': task['name'],\n",
    "            'features': np.array(features),\n",
    "            'labels': task['labels']\n",
    "        }\n",
    "        \n",
    "        task_features.append(task_feature_dict)\n",
    "    \n",
    "    return task_features\n",
    "\n",
    "dinov2_task_features = extract_dinov2_features(task_datasetsDINO)\n",
    "\n",
    "# Printing\n",
    "for task_features in dinov2_task_features:\n",
    "    print(f\"\\nTask: {task_features['name']}\")\n",
    "    print(\"Feature shape:\", task_features['features'].shape)\n",
    "    print(\"Labels shape:\", len(task_features['labels']))\n",
    "    \n",
    "# The code below writes all the feature vectors (for all 60 images) to a txt file\n",
    "\n",
    "with open('dinov2_features.txt', 'w') as file:\n",
    "    for task_features in dinov2_task_features:\n",
    "        file.write(f\"\\nTask: {task_features['name']}\\n\")\n",
    "        file.write(\"Feature Vectors:\\n\")\n",
    "        \n",
    "        for i, (feature, label) in enumerate(zip(task_features['features'], task_features['labels'])):\n",
    "            file.write(f\"Image {i}:\\n\")\n",
    "            file.write(f\"  Label: {label}\\n\")\n",
    "            file.write(f\"  Feature Vector: {feature}\\n\")\n",
    "            file.write(f\"  Vector Length: {len(feature)}\\n\")\n",
    "            file.write(\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihal\\OneDrive\\Desktop\\.vscode\\CompVisionProjectEnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihal\\OneDrive\\Desktop\\.vscode\\CompVisionProjectEnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: Dog vs Cat\n",
      "Feature Vector Shape: (20, 512)\n",
      "\n",
      "Task: Mango vs Banana\n",
      "Feature Vector Shape: (20, 512)\n",
      "\n",
      "Task: Goldfish vs Orca\n",
      "Feature Vector Shape: (20, 512)\n"
     ]
    }
   ],
   "source": [
    "#PROBLEM 3\n",
    "\n",
    "# This code finds all the feature vectors using a CNN method (ResNet).\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class ResNetFeatureExtractor:\n",
    "\n",
    "    #Initializing ResNet model for feature extraction\n",
    "    def __init__(self, model_name='resnet18', pretrained=True):\n",
    "\n",
    "        \n",
    "        # choiosing model\n",
    "        if model_name == 'resnet18':\n",
    "            self.model = torchvision.models.resnet18(pretrained=pretrained)\n",
    "        elif model_name == 'resnet50':\n",
    "            self.model = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet model. Choose 'resnet18' or 'resnet50'.\")\n",
    "        \n",
    "        #removing the classification layer\n",
    "        self.model = torch.nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.model.eval() #evaluation  mode\n",
    "        \n",
    "        # Changing to default resnet size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  \n",
    "                std=[0.229, 0.224, 0.225]    \n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    # Extracting feature vectors for given image paths\n",
    "    def extract_features(self, image_paths):\n",
    "\n",
    "        features = []\n",
    "        \n",
    "        with torch.no_grad():  \n",
    "            for img_path in image_paths:\n",
    "                # Open and transform the image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = self.transform(img).unsqueeze(0)  \n",
    "                \n",
    "                # Extract features\n",
    "                feature = self.model(img_tensor)\n",
    "                \n",
    "                #convert to numpy\n",
    "                feature_vector = feature.squeeze().numpy()\n",
    "                features.append(feature_vector)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "#  Extracting features for all tasks in the dataset and save to a single file\n",
    "def extract_features_for_tasks(task_datasetsCNN, model_name='resnet18', output_file='cnn_features.txt'):\n",
    "\n",
    "    feature_extractor = ResNetFeatureExtractor(model_name=model_name)\n",
    "    \n",
    "    # Writing all features to txt file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for task in task_datasetsCNN:\n",
    "            \n",
    "            task['feature_vectors'] = feature_extractor.extract_features(task['image_paths'])\n",
    "            print(f\"\\nTask: {task['name']}\")\n",
    "            print(\"Feature Vector Shape:\", task['feature_vectors'].shape)\n",
    "            \n",
    "            f.write(f\"Task: {task['name']}\\n\")\n",
    "            f.write(\"Feature Vectors:\\n\")\n",
    "            \n",
    "            for i, (img_path, feature_vector, label) in enumerate(\n",
    "                zip(task['image_paths'], task['feature_vectors'], task['labels'])\n",
    "            ):\n",
    "                f.write(f\"Image {i}:\\n\")\n",
    "                f.write(f\"  Label: {label}\\n\")\n",
    "                f.write(\"  Feature Vector: \")\n",
    "                \n",
    "                \n",
    "                np.set_printoptions(precision=7, suppress=True, linewidth=np.inf)\n",
    "                f.write(np.array2string(feature_vector, separator=', ') + \"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    return task_datasetsCNN\n",
    "\n",
    "# Extract features\n",
    "CNN_task_features = extract_features_for_tasks(task_datasetsCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM 3\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the images\n",
    "# images = glob.glob('image_set/*.jpg')\n",
    "# print(f'Loaded {len(images)} images.')\n",
    "\n",
    "# for fname in images:\n",
    "#     img = cv2.imread(fname)\n",
    "#     print('Processing image %s...' % fname)\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     sift =cv2.SIFT_create()\n",
    "#     keypoints = sift.detect(gray, None)\n",
    "#     # Draw keypoints on the image\n",
    "#     image_with_keypoints = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.title('SIFT Keypoints')\n",
    "#     plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DinoV2 Results:\n",
      "\n",
      "SVM Classification Results for DinoV2 Features:\n",
      "\n",
      "Task: Dog vs Cat\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Test the function for DinoV2 features\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDinoV2 Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m \u001b[43mclassify_with_svm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdinov2_task_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDinoV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Test the function for CNN features\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCNN Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 45\u001b[0m, in \u001b[0;36mclassify_with_svm\u001b[1;34m(task_features, dataset_type)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m stratified_split\u001b[38;5;241m.\u001b[39msplit(features, labels):\n\u001b[0;32m     44\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m features[train_index], features[test_index]\n\u001b[1;32m---> 45\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m, labels[test_index]\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Check if the split resulted in 3 samples per class\u001b[39;00m\n\u001b[0;32m     48\u001b[0m unique_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(y_test)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def classify_with_svm(task_features, dataset_type=\"generic\"):\n",
    "    \"\"\"\n",
    "    Train and evaluate SVM for each classification task using the given features and labels.\n",
    "    Args:\n",
    "        task_features (list): List of dictionaries containing task features and labels.\n",
    "        dataset_type (str): Type of dataset ('DinoV2', 'CNN', or 'generic') to determine key structure.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSVM Classification Results for {dataset_type} Features:\")\n",
    "    \n",
    "    for task in task_features:\n",
    "        print(f\"\\nTask: {task.get('name', 'Unknown Task')}\")\n",
    "        \n",
    "        # Dynamically handle feature extraction based on dataset type\n",
    "        if dataset_type == \"DinoV2\":\n",
    "            features = task.get('features', None)\n",
    "        elif dataset_type == \"CNN\":\n",
    "            features = task.get('feature_vectors', None)\n",
    "        else:\n",
    "            features = task.get('features', task.get('feature_vectors', None))  # Generic handling\n",
    "        \n",
    "        if features is None:\n",
    "            print(\"Error: Features not found in task data!\")\n",
    "            continue\n",
    "        \n",
    "        labels = task.get('labels', None)\n",
    "        if labels is None:\n",
    "            print(\"Error: Labels not found in task data!\")\n",
    "            continue\n",
    "        \n",
    "        # Convert labels to a numpy array if it's not already\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Standardize the features (important for SVM performance)\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "        \n",
    "        # Perform Stratified Split to ensure 3 samples from each class in the test set\n",
    "        stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "        \n",
    "        for train_index, test_index in stratified_split.split(features, labels):\n",
    "            X_train, X_test = features[train_index], features[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "        # Check if the split resulted in 3 samples per class\n",
    "        unique_classes = set(y_test)\n",
    "        class_counts = {class_: list(y_test).count(class_) for class_ in unique_classes}\n",
    "        print(f\"Class distribution in the test set: {class_counts}\")\n",
    "        \n",
    "        # Train the SVM\n",
    "        svm = SVC(kernel='linear', random_state=42)\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = svm.predict(X_test)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=[str(label) for label in set(labels)]))\n",
    "\n",
    "# Test the function for DinoV2 features\n",
    "print(\"DinoV2 Results:\")\n",
    "classify_with_svm(dinov2_task_features, dataset_type=\"DinoV2\")\n",
    "\n",
    "# Test the function for CNN features\n",
    "print(\"\\nCNN Results:\")\n",
    "classify_with_svm(CNN_task_features, dataset_type=\"CNN\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
