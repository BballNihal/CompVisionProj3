{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 1: \n",
    "\n",
    "An image feature is simply interesting or meaningful areas in an image such as the corners of a house or the peaks of a mountain. They are the most useful areas in an image to give enough information to describe the image. An image feature vector is just the numerical representation of these image features placed in a vector.  Typically many of these interest points would have to be found using something like sift rather than manually looking through an image since some points are not as intuitive as say the corners of a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: Dog vs Cat\n",
      "Image Paths: ['image_set/dog_0.jpg', 'image_set/dog_1.jpg', 'image_set/dog_2.jpg', 'image_set/dog_3.jpg', 'image_set/dog_4.jpg', 'image_set/dog_5.jpg', 'image_set/dog_6.jpg', 'image_set/dog_7.jpg', 'image_set/dog_8.jpg', 'image_set/dog_9.jpg', 'image_set/cat_0.jpg', 'image_set/cat_1.jpg', 'image_set/cat_2.jpg', 'image_set/cat_3.jpg', 'image_set/cat_4.jpg', 'image_set/cat_5.jpg', 'image_set/cat_6.jpg', 'image_set/cat_7.jpg', 'image_set/cat_8.jpg', 'image_set/cat_9.jpg']\n",
      "Labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Total Images: 20\n",
      "\n",
      "Task: Mango vs Banana\n",
      "Image Paths: ['image_set/mango_0.jpg', 'image_set/mango_1.jpg', 'image_set/mango_2.jpg', 'image_set/mango_3.jpg', 'image_set/mango_4.jpg', 'image_set/mango_5.jpg', 'image_set/mango_6.jpg', 'image_set/mango_7.jpg', 'image_set/mango_8.jpg', 'image_set/mango_9.jpg', 'image_set/banana_0.jpg', 'image_set/banana_1.jpg', 'image_set/banana_2.jpg', 'image_set/banana_3.jpg', 'image_set/banana_4.jpg', 'image_set/banana_5.jpg', 'image_set/banana_6.jpg', 'image_set/banana_7.jpg', 'image_set/banana_8.jpg', 'image_set/banana_9.jpg']\n",
      "Labels: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Total Images: 20\n",
      "\n",
      "Task: Goldfish vs Orca\n",
      "Image Paths: ['image_set/goldfish_0.jpg', 'image_set/goldfish_1.jpg', 'image_set/goldfish_2.jpg', 'image_set/goldfish_3.jpg', 'image_set/goldfish_4.jpg', 'image_set/goldfish_5.jpg', 'image_set/goldfish_6.jpg', 'image_set/goldfish_7.jpg', 'image_set/goldfish_8.jpg', 'image_set/goldfish_9.jpg', 'image_set/orca_0.jpg', 'image_set/orca_1.jpg', 'image_set/orca_2.jpg', 'image_set/orca_3.jpg', 'image_set/orca_4.jpg', 'image_set/orca_5.jpg', 'image_set/orca_6.jpg', 'image_set/orca_7.jpg', 'image_set/orca_8.jpg', 'image_set/orca_9.jpg']\n",
      "Labels: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Total Images: 20\n"
     ]
    }
   ],
   "source": [
    "#PROBLEM 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "\n",
    "class ImageDatasetLoader:\n",
    "    def __init__(self, img_dir):\n",
    "        # three 2-class tasks\n",
    "        self.tasks = [\n",
    "            {\n",
    "                'name': 'Dog vs Cat',\n",
    "                'classes': {\n",
    "                    'dog': 1,  # Class A\n",
    "                    'cat': 2   # Class B\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Mango vs Banana',\n",
    "                'classes': {\n",
    "                    'mango': 3,  # Class C\n",
    "                    'banana': 4  # Class D\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Goldfish vs Orca',\n",
    "                'classes': {\n",
    "                    'goldfish': 5,  # Class E\n",
    "                    'orca': 6       # Class F\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        self.img_dir = img_dir\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        Load images for each 2-class task\n",
    "        Returns a list of dictionaries, each containing:\n",
    "        - task name\n",
    "        - image paths\n",
    "        - labels\n",
    "        \"\"\"\n",
    "        all_task_datasets = []\n",
    "        \n",
    "        for task in self.tasks:\n",
    "            image_paths = []\n",
    "            labels = []\n",
    "            \n",
    "            #10 images for each class in the task\n",
    "            for category, label in task['classes'].items():\n",
    "                for i in range(10):\n",
    "                    filename = f\"{category}_{i}.jpg\"\n",
    "                    full_path = os.path.join(self.img_dir, filename)\n",
    "                    \n",
    "                    if os.path.exists(full_path):\n",
    "                        image_paths.append(full_path)\n",
    "                        labels.append(label)\n",
    "                    else:\n",
    "                        print(f\"{filename} not found\")\n",
    "            \n",
    "            task_dataset = {\n",
    "                'name': task['name'],\n",
    "                'image_paths': image_paths,\n",
    "                'labels': labels\n",
    "            }\n",
    "            all_task_datasets.append(task_dataset)\n",
    "        \n",
    "        return all_task_datasets\n",
    "\n",
    "#Load the datasets\n",
    "\n",
    "img_dir = \"image_set/\"\n",
    "dataset_loader = ImageDatasetLoader(img_dir)\n",
    "\n",
    "task_datasets = dataset_loader.load_datasets()\n",
    "\n",
    "# Making dataset copies so that they dont affect each other \n",
    "task_datasetsHAND = task_datasets\n",
    "task_datasetsCNN = task_datasets\n",
    "task_datasetsDINO = task_datasets\n",
    "\n",
    "#Print\n",
    "for task in task_datasets:\n",
    "    print(f\"\\nTask: {task['name']}\")\n",
    "    print(\"Image Paths:\", task['image_paths'])\n",
    "    print(\"Labels:\", task['labels'])\n",
    "    print(\"Total Images:\", len(task['image_paths']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printed above is the array of each image in the dataset followed by an array with the class labels of each respective image. This is done for A vs B, C vs D, and E vs F.\n",
    "\n",
    "Classes 1, 2, 3, 4, 5, and 6 correspond with A, B, C, D, E, F respectively where\n",
    "A = dog, \n",
    "B = cat, \n",
    "C = mango, \n",
    "D = banana, \n",
    "E = goldfish, \n",
    "F = orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nihal/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\nihal/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\nihal/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\nihal/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "Task: Dog vs Cat\n",
      "Feature shape: (20, 384)\n",
      "Labels shape: 20\n",
      "\n",
      "Task: Mango vs Banana\n",
      "Feature shape: (20, 384)\n",
      "Labels shape: 20\n",
      "\n",
      "Task: Goldfish vs Orca\n",
      "Feature shape: (20, 384)\n",
      "Labels shape: 20\n"
     ]
    }
   ],
   "source": [
    "#PROBLEM 3\n",
    "\n",
    "# This code finds all the feature vectors using DINOv2.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "class DinoV2FeatureExtractor:\n",
    "\n",
    "    #initializing\n",
    "    def __init__(self, model_name='dinov2_vits14'):\n",
    "\n",
    "        try:\n",
    "            self.model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "            self.model.eval()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DinoV2 model: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # this is the standard transformation for DinoV2\n",
    "        print(\"---\")\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, image_path):\n",
    "\n",
    "        # Extract features from a single image\n",
    "        # Args:\n",
    "        #     Path to the image file\n",
    "        # Returns:\n",
    "        #     Extracted feature vector\n",
    "\n",
    "        try:\n",
    "            # Open and transform the image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            input_tensor = self.transform(image).unsqueeze(0)\n",
    "            \n",
    "            #Extract features\n",
    "            with torch.no_grad():\n",
    "                features = self.model.get_intermediate_layers(input_tensor, n=1)[0]\n",
    "                features = features.mean(1).squeeze()  \n",
    "            \n",
    "            return features.numpy()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"failed for {image_path}: {e}\")\n",
    "            return np.zeros(768) \n",
    "\n",
    "\n",
    "#Extract DinoV2 features for all tasks\n",
    "\n",
    "def extract_dinov2_features(task_datasetsDINO):\n",
    "\n",
    "    #initializing feature vec\n",
    "    extractor = DinoV2FeatureExtractor()\n",
    "    \n",
    "    task_features = []\n",
    "    \n",
    "    for task in task_datasetsDINO:\n",
    "        #extracting features for tast\n",
    "        features = []\n",
    "        for image_path in task['image_paths']:\n",
    "            task_feature = extractor.extract_features(image_path)\n",
    "            features.append(task_feature)\n",
    "        \n",
    "        #creating a task feature dictionary\n",
    "        task_feature_dict = {\n",
    "            'name': task['name'],\n",
    "            'features': np.array(features),\n",
    "            'labels': task['labels']\n",
    "        }\n",
    "        \n",
    "        task_features.append(task_feature_dict)\n",
    "    \n",
    "    return task_features\n",
    "\n",
    "dinov2_task_features = extract_dinov2_features(task_datasetsDINO)\n",
    "\n",
    "# Printing\n",
    "for task_features in dinov2_task_features:\n",
    "    print(f\"\\nTask: {task_features['name']}\")\n",
    "    print(\"Feature shape:\", task_features['features'].shape)\n",
    "    print(\"Labels shape:\", len(task_features['labels']))\n",
    "    \n",
    "# The code below writes all the feature vectors (for all 60 images) to a txt file\n",
    "\n",
    "with open('dinov2_features.txt', 'w') as file:\n",
    "    for task_features in dinov2_task_features:\n",
    "        file.write(f\"\\nTask: {task_features['name']}\\n\")\n",
    "        file.write(\"Feature Vectors:\\n\")\n",
    "        \n",
    "        for i, (feature, label) in enumerate(zip(task_features['features'], task_features['labels'])):\n",
    "            file.write(f\"Image {i}:\\n\")\n",
    "            file.write(f\"  Label: {label}\\n\")\n",
    "            file.write(f\"  Feature Vector: {feature}\\n\")\n",
    "            file.write(f\"  Vector Length: {len(feature)}\\n\")\n",
    "            file.write(\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNetFeatureExtractor' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task_datasetsCNN\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m CNN_task_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_for_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_datasetsCNN\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mextract_features_for_tasks\u001b[1;34m(task_datasetsCNN, model_name, output_file)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features_for_tasks\u001b[39m(task_datasetsCNN, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet18\u001b[39m\u001b[38;5;124m'\u001b[39m, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_features.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 58\u001b[0m     feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mResNetFeatureExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Writing all features to txt file\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mResNetFeatureExtractor.__init__\u001b[1;34m(self, model_name, pretrained)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet18\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m     \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#removing the classification layer\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mchildren())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m#evaluation  mode\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Changing to default resnet size\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ResNetFeatureExtractor' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "#PROBLEM 3\n",
    "\n",
    "# This code finds all the feature vectors using a CNN method (ResNet).\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class ResNetFeatureExtractor:\n",
    "\n",
    "    #Initializing ResNet model for feature extraction\n",
    "    def __init__(self, model_name='resnet18', pretrained=True):\n",
    "\n",
    "        \n",
    "        # choiosing model\n",
    "        if model_name == 'resnet18':\n",
    "            self.model = torchvision.models.resnet18(pretrained=pretrained)\n",
    "        elif model_name == 'resnet50':\n",
    "            self.model = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet model. Choose 'resnet18' or 'resnet50'.\")\n",
    "        \n",
    "        #removing the classification layer\n",
    "        self.model = torch.nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.model.eval() #evaluation  mode\n",
    "        \n",
    "        # Changing to default resnet size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  \n",
    "                std=[0.229, 0.224, 0.225]    \n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    # Extracting feature vectors for given image paths\n",
    "    def extract_features(self, image_paths):\n",
    "\n",
    "        features = []\n",
    "        \n",
    "        with torch.no_grad():  \n",
    "            for img_path in image_paths:\n",
    "                # Open and transform the image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = self.transform(img).unsqueeze(0)  \n",
    "                \n",
    "                # Extract features\n",
    "                feature = self.model(img_tensor)\n",
    "                \n",
    "                #convert to numpy\n",
    "                feature_vector = feature.squeeze().numpy()\n",
    "                features.append(feature_vector)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "#  Extracting features for all tasks in the dataset and save to a single file\n",
    "def extract_features_for_tasks(task_datasetsCNN, model_name='resnet18', output_file='cnn_features.txt'):\n",
    "\n",
    "    feature_extractor = ResNetFeatureExtractor(model_name=model_name)\n",
    "    \n",
    "    # Writing all features to txt file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for task in task_datasetsCNN:\n",
    "            \n",
    "            task['feature_vectors'] = feature_extractor.extract_features(task['image_paths'])\n",
    "            print(f\"\\nTask: {task['name']}\")\n",
    "            print(\"Feature Vector Shape:\", task['feature_vectors'].shape)\n",
    "            \n",
    "            f.write(f\"Task: {task['name']}\\n\")\n",
    "            f.write(\"Feature Vectors:\\n\")\n",
    "            \n",
    "            for i, (img_path, feature_vector, label) in enumerate(\n",
    "                zip(task['image_paths'], task['feature_vectors'], task['labels'])\n",
    "            ):\n",
    "                f.write(f\"Image {i}:\\n\")\n",
    "                f.write(f\"  Label: {label}\\n\")\n",
    "                f.write(\"  Feature Vector: \")\n",
    "                \n",
    "                \n",
    "                np.set_printoptions(precision=7, suppress=True, linewidth=np.inf)\n",
    "                f.write(np.array2string(feature_vector, separator=', ') + \"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    return task_datasetsCNN\n",
    "\n",
    "# Extract features\n",
    "CNN_task_features = extract_features_for_tasks(task_datasetsCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM 3\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the images\n",
    "# images = glob.glob('image_set/*.jpg')\n",
    "# print(f'Loaded {len(images)} images.')\n",
    "\n",
    "# for fname in images:\n",
    "#     img = cv2.imread(fname)\n",
    "#     print('Processing image %s...' % fname)\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     sift =cv2.SIFT_create()\n",
    "#     keypoints = sift.detect(gray, None)\n",
    "#     # Draw keypoints on the image\n",
    "#     image_with_keypoints = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.title('SIFT Keypoints')\n",
    "#     plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN Results:\n",
      "\n",
      "SVM Classification Results for CNN Features:\n",
      "\n",
      "Task: Dog vs Cat\n",
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         6\n",
      "   macro avg       1.00      1.00      1.00         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n",
      "\n",
      "Task: Mango vs Banana\n",
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         6\n",
      "   macro avg       1.00      1.00      1.00         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n",
      "\n",
      "Task: Goldfish vs Orca\n",
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         6\n",
      "   macro avg       1.00      1.00      1.00         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PROBLEM 4\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def classify_with_svm(task_features, dataset_type=\"generic\"):\n",
    "    \"\"\"\n",
    "    Train and evaluate SVM for each classification task using the given features and labels.\n",
    "    Args:\n",
    "        task_features (list): List of dictionaries containing task features and labels.\n",
    "        dataset_type (str): Type of dataset ('DinoV2', 'CNN', or 'generic') to determine key structure.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"\\nSVM Classification Results for {dataset_type} Features:\")\n",
    "    \n",
    "    for task in task_features:\n",
    "        print(f\"\\nTask: {task.get('name', 'Unknown Task')}\")\n",
    "        \n",
    "        # Dynamically handle feature extraction based on dataset type\n",
    "        if dataset_type == \"DinoV2\":\n",
    "            features = task.get('features', None)\n",
    "        elif dataset_type == \"CNN\":\n",
    "            features = task.get('feature_vectors', None)\n",
    "        else:\n",
    "            features = task.get('features', task.get('feature_vectors', None))  # Generic handling\n",
    "        \n",
    "        if features is None:\n",
    "            print(\"Error: Features not found in task data!\")\n",
    "            continue\n",
    "        \n",
    "        labels = task.get('labels', None)\n",
    "        if labels is None:\n",
    "            print(\"Error: Labels not found in task data!\")\n",
    "            continue\n",
    "        \n",
    "        # Standardize the features (important for SVM performance)\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Train the SVM\n",
    "        svm = SVC(kernel='linear', random_state=42)\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = svm.predict(X_test)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=[str(label) for label in set(labels)]))\n",
    "\n",
    "\n",
    "# Call the function for DinoV2 features\n",
    "print(\"DinoV2 Results:\")\n",
    "classify_with_svm(dinov2_task_features, dataset_type=\"DinoV2\")\n",
    "\n",
    "# Call the function for CNN features\n",
    "print(\"\\nCNN Results:\")\n",
    "classify_with_svm(CNN_task_features, dataset_type=\"CNN\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
